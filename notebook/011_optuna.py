
import gc
import os
import random
import numpy as np
from numpy.core.fromnumeric import prod
import pandas as pd
from tqdm import tqdm

from sklearn.model_selection import KFold, StratifiedKFold
import lightgbm as lgb
from sklearn.metrics import mean_squared_error

import seaborn as sns
import IPython.display as ipD
import matplotlib.pyplot as plt
import matplotlib.patches as ptc

from sklearn.preprocessing import StandardScaler, MinMaxScaler
import sys
from sklearn.metrics import roc_auc_score
from optuna import Trial, create_study

# %%
from pathlib import Path
sys.path.append('../')

from src.dataset import RetailDataset, create_target_from_log, only_payment_session_record, create_payment, annot_category
from src.features.features import CountEncodingBlock, DateBlock, PublicLogBlock, MetaInformationBlock, UserHistoryBlock
from src.model_utils import fit_and_predict, create_predict,fit_lgbm
from src.utils import timer, savefig
# %%
path = Path('../input')
dataset = RetailDataset(file_path=path, thres_sec=10*60)
dataset.prepare_data()

save_dir = Path('../input/tutorial2/')

train_target = pd.read_pickle(save_dir/'train_target.pkl')

train_meta = pd.read_pickle(save_dir/'train_meta.pkl')
test_meta = pd.read_pickle(save_dir/'test_meta.pkl')

train_pub_log = pd.read_pickle(save_dir/'train_pub_log.pkl')
train_pri_log = pd.read_pickle(save_dir/'train_pri_log.pkl')

# train_pub + test_whole_log
public_log = pd.read_pickle(save_dir/'public_log.pkl')

#%%
dataset.public_log = public_log

feat_train, feat_test = pd.DataFrame(), pd.DataFrame()

feature_blocks = [
    *[CountEncodingBlock(column=c) for c in ['hour']],
    DateBlock(),
    PublicLogBlock(dataset),
    MetaInformationBlock(),
    UserHistoryBlock(dataset),
]

for block in feature_blocks:
    with timer(prefix='fit {} '.format(block)):
        out_i = block.fit(train_meta)
    assert len(train_meta) == len(out_i), block
    feat_train = pd.concat([feat_train, out_i], axis=1)

for block in feature_blocks:
    with timer(prefix='fit {} '.format(block)):
        out_i = block.transform(test_meta)

    assert len(test_meta) == len(out_i), block
    feat_test = pd.concat([feat_test, out_i], axis=1)

print(train_target.shape, feat_train.shape)

#%%
TARGET_IDS = dataset.target_category_ids

def get_boosting_parameter_suggestions(trial: Trial) -> dict:
    """
    Get parameter sample for Boosting (like XGBoost, LightGBM)

    Args:
        trial(trial.Trial):

    Returns:
        dict:
            parameter sample generated by trial object
    """
    return {
        # L2 正則化
        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 1e3),

        # L1 正則化
        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 1e3),

        # 弱学習木ごとに使う特徴量の割合
        # 0.5 だと全体のうち半分の特徴量を最初に選んで, その範囲内で木を成長させる
        'feature_fraction': trial.suggest_loguniform('feature_fraction', .5, 1.),

        # 学習データ全体のうち使用する割合
        # colsample とは反対に row 方向にサンプルする
        'bagging_fraction': trial.suggest_loguniform('bagging_fraction', .5, 1.),

        # 'colsample_bytree': trial.suggest_loguniform('colsample_bytree', .5, 1.),

        # 木の最大の深さ
        # たとえば 5 の時各弱学習木のぶん機は最大でも5に制限される.
        'max_depth': trial.suggest_int('max_depth', low=3, high=8),

        'num_leaves': trial.suggest_int('num_leaves', low=8, high=256),

        # 末端ノードに含まれる最小のサンプル数
        # これを下回るような分割は作れなくなるため, 大きく設定するとより全体の傾向でしか分割ができなくなる
        # [NOTE]: 数であるのでデータセットの大きさ依存であることに注意
        'min_child_weight': trial.suggest_uniform('min_child_weight', low=.5, high=40)
    }


def objective(trial: Trial) -> float:
    """目的関数.
    Trial instance をうけとって, その trial で作ったパラメータの評価値 (objective) の値を返します. """

    params = get_boosting_parameter_suggestions(trial)

    # 実際に lightGBM にわたす parameter に変換.
    # この method を切り出しているのはあとで best trial の parameter 使って学習するときにもこの変換を使うため.
    params = to_lgbm_params(params)

    # 今はすべての fold での検証結果で Out-of-fold score の見積もりをしているけれど、それなりに時間がかかる場合には
    # 一番はじめの fold だけでスコアを出す場合もある
    # 当然分散もあるため評価は不正確になる, この辺は時間とのトレードオフ
    score_list = []
    for target_id in TARGET_IDS:
        cv = StratifiedKFold(n_splits=2, random_state=71, shuffle=True)
        X = feat_train.values
        y = train_target[target_id].values
        y = np.where(y > 0, 1, 0)

        _, oof, score = fit_lgbm(X, y, params=params,cv=cv, verbose=None)
        score_list.append(score)

    return np.array(score_list).mean()


def to_lgbm_params(params: dict):
    retval = dict(**params)
    retval.update({
        'n_estimators': 10000,
        'learning_rate': .1,
        'objective': 'binary',
        # 'metric': 'rmse',
        'importance_type': 'gain',
        # 'verbose': -1
    })
    return retval


study = create_study(direction='maximize')
study.optimize(objective,
                timeout = 60 * 60)
            #    n_trials=200)  # 時間の関係上1回にしてる

# models, oof = fit_lgbm(X, y, params=to_lgbm_params(study.best_params))
print(study.best_params,study.best_value)
